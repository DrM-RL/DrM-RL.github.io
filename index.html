<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>DrM: Mastering Visual Reinforcement Learning through Dormant Ratio Minimization</title>
<meta name="description" content="Abstract">
<link rel="stylesheet" href="/css/style.css">
<link rel="canonical" href="https://drm-rl.github.io/">
<link rel="shortcut icon" href="asset/favicon.jpg"> 
</head>
<body>

<main>
<article class="project">
<div class="cover">
<div class="wrapper">
<h1>DrM: Mastering Visual Reinforcement Learning through Dormant Ratio Minimization</h1>
<p class="authors">
<span class="author"><a target="_blank" href="https://drm-rl.github.io/">Anonymous</a></span> 
</p>
<!--<p class="venues">Preprint</p>-->

<!--<div class="image"><img src="asset/title.gif" /></div>-->
<!--<p class="buttons">
<a href="https://arxiv.org/" target="_blank">Paper</a>
<a href="https://twitter.com/" target="_blank">Twitter</a>
<a href="https://github.com/" target="_blank">Code</a>
</p>-->
</div>
</div>

<div class="content wrapper">
  
<h2 id="abstract">Abstract</h2>
<p>Visual reinforcement learning (RL) has shown promise in continuous control tasks.
  Despite its progress, current algorithms are still unsatisfactory in virtually every
  aspect of the performance such as sample efficiency, asymptotic performance, and
  their robustness to the choice of random seeds. In this paper, we identify a major
  shortcoming in existing visual RL methods that is the agents often exhibit sustained
  inactivity during early training, thereby limiting their ability to explore effectively.
  Expanding upon this crucial observation, we additionally unveil a significant correlation between the agents' inclination towards motorically inactive exploration and
  the absence of neuronal activity within their policy networks. To quantify this inactivity, we adopt dormant ratio as a metric to measure inactivity
  in the RL agent's network. Empirically, we also recognize that the dormant ratio
  can act as a standalone indicator of an agent's activity level, regardless of the received reward signals. Leveraging the aforementioned insights, we introduce DrM ,
  a method that uses three core mechanisms to guide agents' exploration-exploitation
  trade-offs by actively minimizing the dormant ratio. Experiments demonstrate that
  DrM achieves significant improvements in sample efficiency and asymptotic performance with no broken seeds (76 seeds in total) across three continuous control
  benchmark environments, including DeepMind Control Suite, MetaWorld, and
  Adroit. Most importantly, DrM is the first model-free algorithm that consistently
  solves tasks in both the Dog and Manipulator domains from the DeepMind Control
  Suite as well as three dexterous hand manipulation tasks without demonstrations in
  Adroit, all based on pixel observations.</p>


<h2 id="highlights">Highlights</h2>
<p style="text-align: center">
  <img style="width: 78%" src="/asset/drm/all.png" />
  </p>
<p>We evaluate DrM on three benchmarks for both locomotion and robotic manipulation: DeepMind Control Suite, Meta-World, and Adroit. 
    These environments feature rich visual elements like textures and shading, necessitate fine-grained control due to complex geometry, and introduce additional challenges 
    such as sparse rewards and high-dimensional action spaces that previous visual RL algorithms like DrQv2 have been unable to solve.</p>
<!--<video controls="" muted="" style="width: 100%; border-radius: .25em">
  <source src="/asset/drm/diamond.mp4" />
</video>-->


<h2 id="dmc">DeepMind Control Suite</h2>
<p><img style="width: 100%" src="/asset/drm/dmc.png" /></p>
<p>For Deepmind Control Suite, we evaluate DrM on eight hardest tasks from the Humanoid, Dog, and Manipulator domain, as well as Acrobot Swingup Sparse.. 
    The Manipulator domain is particularly challenging due to its sparse reward structure and the long horizon required for skill acquisition, 
    while Humanoid and Dog tasks feature intricate kinematics, skinning weights, collision geometry, as well as muscle and tendon attachment points. </p>
    <div class="videos">
      <video controls="" muted="" autoplay="" loop=""><source src="/asset/dmc/dog.mp4" /></video>
      <video controls="" muted="" autoplay="" loop=""><source src="/asset/dmc/manipulator.mp4" /></video>
      <video controls="" muted="" autoplay="" loop=""><source src="/asset/dmc/humanoid.mp4" /></video>
      </div>
<style>
.videos { display: flex; flex-wrap: wrap; align-content: space-between; gap: .5em; }
.videos video { flex: 1; width: 23%; border-radius: .25em; }
.restart { display: inline-block; padding: .55em 1em .45em; border-radius: .25em; background: #ddd; cursor: pointer; }
.restart:hover { background: #eee; }
.restart:active { background: #ccc; }
</style>
<script defer="">
function restartVideos() {
  document.querySelectorAll('.videos video').forEach((video) => {
    video.pause()
    video.currentTime = 0
    video.play()
  })
}
</script>
<p style="text-align: center">
<span class="restart" onclick="restartVideos()">Restart Videos</span>
</p>


<h2 id="scaling">Meta-World</h2>
<p><img style="width: 100%" src="/asset/drm/metaworld.png" /></p>
<p> In Meta-World, we evaluate DrM and baselines on eight challenging tasks
  including 4 very hard tasks with dense rewards following prior works and 4 medium tasks with sparse
  success signals. </p>
<div class="videos">
  <video controls="" muted="" autoplay="" loop=""><source src="asset/metaworld/coffee-push.gif" /></video>
  <video controls="" muted="" autoplay="" loop=""><source src="asset/metaworld/disassemble.gif" /></video>
  <video controls="" muted="" autoplay="" loop=""><source src="asset/metaworld/pick-place-wall.gif" /></video>
  <video controls="" muted="" autoplay="" loop=""><source src="asset/metaworld/soccer.gif" /></video>
  <video controls="" muted="" autoplay="" loop=""><source src="asset/metaworld/stick-pull.gif" /></video>
  <video controls="" muted="" autoplay="" loop=""><source src="asset/metaworld/sweep-into.gif" /></video>
 </div>

<style>
  .videos { display: flex; flex-wrap: wrap; align-content: space-between; gap: .5em; }
  .videos video { flex: 1; width: 23%; border-radius: .25em; }
  .restart { display: inline-block; padding: .55em 1em .45em; border-radius: .25em; background: #ddd; cursor: pointer; }
  .restart:hover { background: #eee; }
  .restart:active { background: #ccc; }
  </style>
  <script defer="">
  function restartVideos() {
    document.querySelectorAll('.videos video').forEach((video) => {
      video.pause()
      video.currentTime = 0
      video.play()
    })
  }
  </script>
  <p style="text-align: center">
  <span class="restart" onclick="restartVideos()">Restart Videos</span>
</p>


<h2 id="scaling">Adroit</h2>
<p><img style="width: 100%" src="/asset/drm/adroit.png" /></p>
<p>We also evaluate DrM on the Adroit domain, focusing on three dexterous hand manipulation tasks: Hammer, Door, and Pen, 
  which requires controlling a robotic hand with 24 degrees of freedom. Notably, DrM is the first documented model-free visual 
  RL algorithm capable of reliably solving tasks in the Adroit domain without expert demonstrations.  </p>
<!--<div class="videos">
  <video controls="" muted="" autoplay="" loop=""><source src="https://user-images.githubusercontent.com/2111293/212325343-cb0a71ba-b8a0-4c81-a5a8-0b7a738b6117.mp4" /></video>
  <video controls="" muted="" autoplay="" loop=""><source src="https://user-images.githubusercontent.com/2111293/212325346-1dadce21-a44e-4dfa-bc0b-72d8edca01d7.mp4" /></video>
  <video controls="" muted="" autoplay="" loop=""><source src="https://user-images.githubusercontent.com/2111293/212325349-aad730cc-12dd-4f63-a0c2-7b70706b254a.mp4" /></video>
  <video controls="" muted="" autoplay="" loop=""><source src="https://user-images.githubusercontent.com/2111293/212325327-93ca12d1-f268-477a-b396-337db213372d.mp4" /></video>
</div>-->
<!--<style>
  .videos { display: flex; flex-wrap: wrap; align-content: space-between; gap: .5em; }
  .videos video { flex: 1; width: 23%; border-radius: .25em; }
  .restart { display: inline-block; padding: .55em 1em .45em; border-radius: .25em; background: #ddd; cursor: pointer; }
  .restart:hover { background: #eee; }
  .restart:active { background: #ccc; }
</style>
<script defer="">
  function restartVideos() {
    document.querySelectorAll('.videos video').forEach((video) => {
      video.pause()
      video.currentTime = 0
      video.play()
    })
  }
</script>
<p style="text-align: center">
  <span class="restart" onclick="restartVideos()">Restart Videos</span>
</p>-->


<h2 id="scaling">Dormant Ratio</h2>
<p>There is a connection between the sharp reduction of an agent's dormant ratio and the agent's skill acquisition in visual continuous control tasks. </p>
<!--<div class="videos">
  <video controls="" muted="" autoplay="" loop=""><source src="https://user-images.githubusercontent.com/2111293/212325343-cb0a71ba-b8a0-4c81-a5a8-0b7a738b6117.mp4" /></video>
  <video controls="" muted="" autoplay="" loop=""><source src="https://user-images.githubusercontent.com/2111293/212325346-1dadce21-a44e-4dfa-bc0b-72d8edca01d7.mp4" /></video>
  <video controls="" muted="" autoplay="" loop=""><source src="https://user-images.githubusercontent.com/2111293/212325349-aad730cc-12dd-4f63-a0c2-7b70706b254a.mp4" /></video>
  <video controls="" muted="" autoplay="" loop=""><source src="https://user-images.githubusercontent.com/2111293/212325327-93ca12d1-f268-477a-b396-337db213372d.mp4" /></video>
</div>
<style>
  .videos { display: flex; flex-wrap: wrap; align-content: space-between; gap: .5em; }
  .videos video { flex: 1; width: 23%; border-radius: .25em; }
  .restart { display: inline-block; padding: .55em 1em .45em; border-radius: .25em; background: #ddd; cursor: pointer; }
  .restart:hover { background: #eee; }
  .restart:active { background: #ccc; }
</style>
<script defer="">
  function restartVideos() {
    document.querySelectorAll('.videos video').forEach((video) => {
      video.pause()
      video.currentTime = 0
      video.play()
    })
  }
</script>
<p style="text-align: center">
  <span class="restart" onclick="restartVideos()">Restart Videos</span>
</p>-->

<h2 id="data-efficiency">Time Efficiency</h2>
<p>To assess the algorithms' speed, we measure their frames per second (FPS) on the same DeepMind Control Suite task, Dog Walk, using an identical Nvidia RTX A5000 GPU. 
  While achieving significant sample efficiency and asymptotic performance, DrM only slightly compromises wall-clock time compared to DrQ-v2. 
  Compared with two other baselines, DrM is roughly as time-efficient as ALIX and about three times faster than TACO.</p>
<!--<div class="videos dmlab">
<video controls="" muted="" autoplay="" loop=""><source src="https://user-images.githubusercontent.com/2111293/212435069-0845fe09-b3d8-4518-9172-daaac5092b2d.mp4" /></video>
<video controls="" muted="" autoplay="" loop=""><source src="https://user-images.githubusercontent.com/2111293/212435059-6ff4d419-abf6-49ae-890b-210c29a7d470.mp4" /></video>
<video controls="" muted="" autoplay="" loop=""><source src="https://user-images.githubusercontent.com/2111293/212435049-5d66a543-8dba-42ff-83fe-50774e6c8cda.mp4" /></video>
<video controls="" muted="" autoplay="" loop=""><source src="https://user-images.githubusercontent.com/2111293/212435043-419af193-9502-4d1b-8417-16358653a156.mp4" /></video>
</div>
<script defer="">
document.querySelectorAll('.videos.dmlab video').forEach((video) => {
  video.playbackRate = 0.5
})
</script>-->
<p style="text-align: center">
<img style="width: 40%" src="/asset/drm/fps.png" />
</p>
</div>
</article>

<script>
document.querySelector('body').onload = function() {
  console.log('SCROLL', window.scrollY);
  if (window.scrollY > 10) return;
  const element = document.querySelector('.cover')
  const pos = element.getBoundingClientRect().top + window.scrollY
  window.scroll({top: pos, behavior: 'smooth'})
}
</script>
</main>

<footer></footer>
<script>
window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
ga('create', 'UA-78082517-1', 'auto');
ga('send', 'pageview');
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>
<script async src='/scripts.js'></script>
</body>
</html>