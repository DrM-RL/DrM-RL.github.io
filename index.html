<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>DrM: Mastering Visual Reinforcement Learning through Dormant Ratio Minimization</title>
<meta name="description" content="Abstract">
<link rel="stylesheet" href="/css/style.css">
<link rel="canonical" href="https://drm-rl.github.io/">
<link rel="shortcut icon" href="asset/favicon.jpg"> 
</head>
<body>

<main>
<article class="project">
<div class="cover">
<div class="wrapper">
<h1>DrM: Mastering Visual Reinforcement Learning through Dormant Ratio Minimization</h1>
<p class="authors">
<span class="author"><a target="_blank" href="https://drm-rl.github.io/">Anonymous</a></span> 
</p>
<!--<p class="venues">Preprint</p>-->

<!--<div class="image"><img src="asset/title.gif" /></div>-->
<!--<p class="buttons">
<a href="https://arxiv.org/" target="_blank">Paper</a>
<a href="https://twitter.com/" target="_blank">Twitter</a>
<a href="https://github.com/" target="_blank">Code</a>
</p>-->
</div>
</div>

<div class="content wrapper">
  
<h2 id="abstract">Abstract</h2>
<p>Visual Reinforcement Learning (Visual RL) has shown promise in continuous control tasks, yet faces challenges related to effective exploration coupled with representation learning and credit assignment problems. 
    Consequently, these challenges make current algorithms particularly sensitive to the choice of random seeds and limit their use to relatively simple tasks.
    In this paper, we identify a major shortcoming in existing Visual RL methods: agents often show sustained inactivity during early training, which restricts their ability to explore effectively. 
    To quantify this inactivity, we utilize dormant ratio, a metric originally designed for measuring inactivity in RL agent's network.
    Our empirical analysis confirms a significant relationship between the dormant ratio and the agent's propensity for active exploration. 
    Interestingly, we find that the dormant ratio can act as a standalone indicator of an agent's activity level, regardless of the received reward signals.
    Leveraging this crucial insight, we introduce DrM that aims to actively minimize the dormant ratio while utilizing dormant ratio to guide the agent's decisions on the exploration-exploitation trade-off. 
    Empirically, DrM achieves significant improvements in both sample efficiency and asymptotic performance across three continuous control benchmark environments, including DeepMind Control Suite, Metaworld, and Adroit. 
    Most importantly, DrM is the first model-free algorithm that consistently solves tasks in both the Dog and Manipulator domains from the DeepMind Control Suite as well as three dexterous hand manipulation tasks in Adroit, all based on pixel observations.</p>


<h2 id="highlights">Highlights</h2>
<p style="text-align: center">
  <img style="width: 78%" src="/asset/drm/all.png" />
  </p>
<p>We evaluate DrM on three benchmarks for both locomotion and robotic manipulation: DeepMind Control Suite, Meta-World, and Adroit. 
    These environments feature rich visual elements like textures and shading, necessitate fine-grained control due to complex geometry, and introduce additional challenges 
    such as sparse rewards and high-dimensional action spaces that previous visual RL algorithms like DrQv2 have been unable to solve.</p>
<!--<video controls="" muted="" style="width: 100%; border-radius: .25em">
  <source src="/asset/drm/diamond.mp4" />
</video>-->


<h2 id="dmc">DeepMind Control Suite</h2>
<p>For Deepmind Control Suite, we evaluate DrM on eight hardest tasks from the Humanoid, Dog, and Manipulator domain. 
    The Manipulator domain is particularly challenging due to its sparse reward structure and the long horizon required for skill acquisition, 
    while Humanoid and Dog tasks feature intricate kinematics, skinning weights, collision geometry, as well as muscle and tendon attachment points. </p>
    <!--<div class="videos">
      <video controls="" muted="" autoplay="" loop=""><source src="https://user-images.githubusercontent.com/2111293/212325353-fb56ff5b-50a3-4a32-a854-35c6d8cbb490.mp4" /></video>
      <video controls="" muted="" autoplay="" loop=""><source src="https://user-images.githubusercontent.com/2111293/212325324-c3eba6e8-6744-404f-8f8c-c8838e244627.mp4" /></video>
      <video controls="" muted="" autoplay="" loop=""><source src="https://user-images.githubusercontent.com/2111293/212325328-64038edf-5955-487a-878b-79fb6aaf081b.mp4" /></video>
      <video controls="" muted="" autoplay="" loop=""><source src="https://user-images.githubusercontent.com/2111293/212325331-8f0e08d8-805a-4fd1-8778-50bf89b1d172.mp4" /></video>
      </div>-->
<p>This complexity makes the three domains extremely difficult for algorithms to learn and control effectively.
  We note that DrM is the first documented model-free visual RL algorithm capable of solving both Dog and Manipulator domains in the DeepMind Control Suite using pixel observations. 
</p>
<p><img style="width: 100%" src="/asset/drm/dmc.png" /></p>
<!--<style>
.videos { display: flex; flex-wrap: wrap; align-content: space-between; gap: .5em; }
.videos video { flex: 1; width: 23%; border-radius: .25em; }
.restart { display: inline-block; padding: .55em 1em .45em; border-radius: .25em; background: #ddd; cursor: pointer; }
.restart:hover { background: #eee; }
.restart:active { background: #ccc; }
</style>
<script defer="">
function restartVideos() {
  document.querySelectorAll('.videos video').forEach((video) => {
    video.pause()
    video.currentTime = 0
    video.play()
  })
}
</script>
<p style="text-align: center">
<span class="restart" onclick="restartVideos()">Restart Videos</span>
</p>-->


<h2 id="scaling">Meta-World</h2>
<p>In MetaWorld, we evaluate our method and baselines on six challenging tasks with dense human-shaping rewards following prior works, including one medium, two hard, and three very hard tasks.
  Across this spectrum of tasks with varying degrees of complexity, our method consistently outperformed the baseline algorithms in terms of sample efficiency.</p>
<!--<div class="videos">
  <video controls="" muted="" autoplay="" loop=""><source src="https://user-images.githubusercontent.com/2111293/212325333-7f2c0e5f-f862-45f5-8399-93aa2d94f2fb.mp4" /></video>
  <video controls="" muted="" autoplay="" loop=""><source src="https://user-images.githubusercontent.com/2111293/212325334-5a44904c-4e6b-4ffe-befe-a2b99c1f5279.mp4" /></video>
  <video controls="" muted="" autoplay="" loop=""><source src="https://user-images.githubusercontent.com/2111293/212325338-2c8e22a9-e3e1-4099-a015-544936583922.mp4" /></video>
  <video controls="" muted="" autoplay="" loop=""><source src="https://user-images.githubusercontent.com/2111293/212325340-4d580209-6013-4fbd-aa52-e7bd3588c229.mp4" /></video>
</div>-->
<p>Moreover, in more challenging scenarios where we only used sparse task completion rewards for three medium tasks, most other baselines struggled to learn using spare rewards, 
  with success rates hovering close to zero. In contrast, our algorithm achieve success rates ranging from 20% to 40%, highlighting the substantial benefits of dormant-ratio-based 
  exploration for sparse reward tasks.
</p>
<p><img style="width: 100%" src="/asset/drm/metaworld.png" /></p>
<!--<style>
  .videos { display: flex; flex-wrap: wrap; align-content: space-between; gap: .5em; }
  .videos video { flex: 1; width: 23%; border-radius: .25em; }
  .restart { display: inline-block; padding: .55em 1em .45em; border-radius: .25em; background: #ddd; cursor: pointer; }
  .restart:hover { background: #eee; }
  .restart:active { background: #ccc; }
  </style>
  <script defer="">
  function restartVideos() {
    document.querySelectorAll('.videos video').forEach((video) => {
      video.pause()
      video.currentTime = 0
      video.play()
    })
  }
  </script>
  <p style="text-align: center">
  <span class="restart" onclick="restartVideos()">Restart Videos</span>
</p>-->


<h2 id="scaling">Adroit</h2>
<p>We also evaluate DrM on the Adroit domain, focusing on three dexterous hand manipulation tasks: Hammer, Door, and Pen, 
  which requires controlling a robotic hand with 24 degrees of freedom. Given the task's high-dimensional action space and 
  intricate physics, previous reinforcement learning algorithms have faced significant challenges, epscially when learning from pixel observations. </p>
<!--<div class="videos">
  <video controls="" muted="" autoplay="" loop=""><source src="https://user-images.githubusercontent.com/2111293/212325343-cb0a71ba-b8a0-4c81-a5a8-0b7a738b6117.mp4" /></video>
  <video controls="" muted="" autoplay="" loop=""><source src="https://user-images.githubusercontent.com/2111293/212325346-1dadce21-a44e-4dfa-bc0b-72d8edca01d7.mp4" /></video>
  <video controls="" muted="" autoplay="" loop=""><source src="https://user-images.githubusercontent.com/2111293/212325349-aad730cc-12dd-4f63-a0c2-7b70706b254a.mp4" /></video>
  <video controls="" muted="" autoplay="" loop=""><source src="https://user-images.githubusercontent.com/2111293/212325327-93ca12d1-f268-477a-b396-337db213372d.mp4" /></video>
</div>-->
<p>Notably, DrM is the first documented model-free visual RL algorithm capable of reliably solving tasks in the Adroit domain without expert demonstrations.</p>
<p><img style="width: 100%" src="/asset/drm/adroit.png" /></p>
<!--<style>
  .videos { display: flex; flex-wrap: wrap; align-content: space-between; gap: .5em; }
  .videos video { flex: 1; width: 23%; border-radius: .25em; }
  .restart { display: inline-block; padding: .55em 1em .45em; border-radius: .25em; background: #ddd; cursor: pointer; }
  .restart:hover { background: #eee; }
  .restart:active { background: #ccc; }
</style>
<script defer="">
  function restartVideos() {
    document.querySelectorAll('.videos video').forEach((video) => {
      video.pause()
      video.currentTime = 0
      video.play()
    })
  }
</script>
<p style="text-align: center">
  <span class="restart" onclick="restartVideos()">Restart Videos</span>
</p>-->


<h2 id="scaling">Dormant Ratio</h2>
<p>There is a connection between the sharp reduction of an agent's dormant ratio and the agent's skill acquisition in visual continuous control tasks. </p>
<!--<div class="videos">
  <video controls="" muted="" autoplay="" loop=""><source src="https://user-images.githubusercontent.com/2111293/212325343-cb0a71ba-b8a0-4c81-a5a8-0b7a738b6117.mp4" /></video>
  <video controls="" muted="" autoplay="" loop=""><source src="https://user-images.githubusercontent.com/2111293/212325346-1dadce21-a44e-4dfa-bc0b-72d8edca01d7.mp4" /></video>
  <video controls="" muted="" autoplay="" loop=""><source src="https://user-images.githubusercontent.com/2111293/212325349-aad730cc-12dd-4f63-a0c2-7b70706b254a.mp4" /></video>
  <video controls="" muted="" autoplay="" loop=""><source src="https://user-images.githubusercontent.com/2111293/212325327-93ca12d1-f268-477a-b396-337db213372d.mp4" /></video>
</div>
<style>
  .videos { display: flex; flex-wrap: wrap; align-content: space-between; gap: .5em; }
  .videos video { flex: 1; width: 23%; border-radius: .25em; }
  .restart { display: inline-block; padding: .55em 1em .45em; border-radius: .25em; background: #ddd; cursor: pointer; }
  .restart:hover { background: #eee; }
  .restart:active { background: #ccc; }
</style>
<script defer="">
  function restartVideos() {
    document.querySelectorAll('.videos video').forEach((video) => {
      video.pause()
      video.currentTime = 0
      video.play()
    })
  }
</script>
<p style="text-align: center">
  <span class="restart" onclick="restartVideos()">Restart Videos</span>
</p>-->

<h2 id="data-efficiency">Time Efficiency</h2>
<p>To assess the algorithms' speed, we measure their frames per second (FPS) on the same DeepMind Control Suite task, Dog Walk, using an identical Nvidia RTX A5000 GPU. 
  While achieving significant sample efficiency and asymptotic performance, DrM only slightly compromises wall-clock time compared to DrQ-v2. 
  Compared with two other baselines, DrM is roughly as time-efficient as ALIX and about three times faster than TACO.</p>
<!--<div class="videos dmlab">
<video controls="" muted="" autoplay="" loop=""><source src="https://user-images.githubusercontent.com/2111293/212435069-0845fe09-b3d8-4518-9172-daaac5092b2d.mp4" /></video>
<video controls="" muted="" autoplay="" loop=""><source src="https://user-images.githubusercontent.com/2111293/212435059-6ff4d419-abf6-49ae-890b-210c29a7d470.mp4" /></video>
<video controls="" muted="" autoplay="" loop=""><source src="https://user-images.githubusercontent.com/2111293/212435049-5d66a543-8dba-42ff-83fe-50774e6c8cda.mp4" /></video>
<video controls="" muted="" autoplay="" loop=""><source src="https://user-images.githubusercontent.com/2111293/212435043-419af193-9502-4d1b-8417-16358653a156.mp4" /></video>
</div>
<script defer="">
document.querySelectorAll('.videos.dmlab video').forEach((video) => {
  video.playbackRate = 0.5
})
</script>-->
<p style="text-align: center">
<img style="width: 40%" src="/asset/drm/fps.png" />
</p>
</div>
</article>

<script>
document.querySelector('body').onload = function() {
  console.log('SCROLL', window.scrollY);
  if (window.scrollY > 10) return;
  const element = document.querySelector('.cover')
  const pos = element.getBoundingClientRect().top + window.scrollY
  window.scroll({top: pos, behavior: 'smooth'})
}
</script>
</main>

<footer></footer>
<script>
window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
ga('create', 'UA-78082517-1', 'auto');
ga('send', 'pageview');
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>
<script async src='/scripts.js'></script>
</body>
</html>